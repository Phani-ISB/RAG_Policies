# -*- coding: utf-8 -*-
"""RAG with policies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yupRh8EVZCDaIc_kFMdKIU0pcXL50nVr

# import all the dependencies
"""

"""
Streamlit RAG chatbot using LlamaIndex (llama_index) for indexing + retrieval
and Google Generative API for generation.

Features:
- Upload up to 2 PDFs in the UI (or point to a local folder if you prefer)
- Builds index (persisted) using sentence-transformers embeddings
- Uses Google Generative API (PaLM / Vertex AI Generative) as the LLM
- Simple conversational UI with retrieval + generation (RAG)

Environment variables required before running/deploying:
- GOOGLE_API_KEY (or set GOOGLE_APPLICATION_CREDENTIALS pointing to a service account json)
- STREAMLIT_SERVER_ENABLE_CORS = "false" (optional when deploying)

Install dependencies (recommended inside venv):

pip install --upgrade pip
pip install streamlit llama-index sentence-transformers faiss-cpu google-generative-ai PyPDF2

Notes:
- This is a minimal example meant to run on Streamlit Cloud. You may need to adjust
  the Google Generative API code depending on your Google client library version.
- Persisted index is stored in './index_store/'. On Streamlit Cloud use session-only storage
  (persisting across deploys requires external storage like S3 or GCS).

"""

import os
import tempfile
from typing import List

import streamlit as st
from PyPDF2 import PdfReader

# llama-index imports
from llama_index import (
    GPTVectorStoreIndex,
    SimpleDirectoryReader,
    ServiceContext,
    LLMPredictor,
    StorageContext,
    load_index_from_storage,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores import SimpleVectorStore

# sentence-transformers for embeddings (used by HuggingFaceEmbedding)
# google generative ai client
try:
    import google.genai as genai
except Exception:
    genai = None

# ----------------------------
# Helper: text extraction
# ----------------------------

def extract_text_from_pdf(file) -> str:
    reader = PdfReader(file)
    text_pages = []
    for p in reader.pages:
        try:
            text = p.extract_text() or ""
        except Exception:
            text = ""
        if text:
            text_pages.append(text)
    return "\n\n".join(text_pages)


# ----------------------------
# Google Generative wrapper for LlamaIndex's LLMPredictor
# ----------------------------
class GoogleGenerativeLLM:
    """Very small wrapper around google.generativeai to match LlamaIndex expectations.

    This wrapper implements a `predict` method used by llama-index's LLMPredictor.
    If your environment uses a different Google SDK or a different method name, update accordingly.
    """

    def __init__(self, api_key: str = None, model: str = "models/text-bison-001"):
        self.api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        if not self.api_key :
            print(" Warning : Set your Google API KEY")
            self.api_key = None
        self.model = model

    def generate(self, prompt: str, max_output_tokens: int = 512) -> str:
        # call the Google generative api
        response = genai.generate_text(model=self.model, prompt=prompt, max_output_tokens=max_output_tokens)
        # response object may differ slightly depending on client version
        # common interface: response.text or response.candidates[0].output
        if hasattr(response, "text") and response.text:
            return response.text
        # fallback
        try:
            return response.candidates[0].output
        except Exception:
            return str(response)


# LlamaIndex expects an LLMPredictor wrapper object with a `predict` method or something similar.
# We'll make a small adapter.
class LLMPredictorGoogle:
    def __init__(self, ggllm: GoogleGenerativeLLM, max_output_tokens: int = 512):
        self.ggllm = ggllm
        self.max_output_tokens = max_output_tokens

    def predict(self, prompt: str, **kwargs) -> str:
        return self.ggllm.generate(prompt, max_output_tokens=self.max_output_tokens)


# ----------------------------
# Build / load index helper
# ----------------------------

def build_index_from_texts(texts: List[str], persist_dir: str = "./index_store") -> GPTVectorStoreIndex:
    os.makedirs(persist_dir, exist_ok=True)

    # Use a HuggingFace embedding model via llama-index adapter
    # This uses sentence-transformers under the hood.
    hf_embed = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # Create service context with embeddings - default predictor will be set later
    service_context = ServiceContext.from_defaults(embed_model=hf_embed)

    # make documents
    from llama_index import Document

    documents = [Document(text=t) for t in texts]

    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)

    # persist
    index.storage_context.persist(persist_dir)
    return index


def load_or_build_index(persist_dir: str, texts: List[str]):
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        try:
            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
            index = load_index_from_storage(storage_context=storage_context)
            return index
        except Exception:
            # fallback: rebuild
            pass
    return build_index_from_texts(texts, persist_dir=persist_dir)


# ----------------------------
# Streamlit app
# ----------------------------

st.set_page_config(page_title="RAG Chatbot - LlamaIndex + Google Generative", layout="wide")
st.title("RAG chatbot using LlamaIndex + Google Generative API")

st.markdown(
    "Upload up to 2 PDFs. We'll build a vector index from their text and let you ask questions (retrieval-augmented generation)."
)

with st.sidebar:
    st.header("Settings")
    persist_dir = st.text_input("Index persist dir (on Streamlit Cloud this is ephemeral)", value="./index_store")
    model_choice = st.selectbox("Google model", options=["models/text-bison-001", "models/chat-bison-001"], index=0)
    max_tokens = st.slider("Max response tokens", min_value=64, max_value=1024, value=512)

uploaded_files = st.file_uploader("Upload PDF files (up to 2)", type=["pdf"], accept_multiple_files=True)

index_ready = False
if uploaded_files and len(uploaded_files) > 0:
    if len(uploaded_files) > 2:
        st.warning("Please upload at most 2 PDFs for this demo.")
    else:
        with st.spinner("Extracting text from PDFs..."):
            texts = []
            for f in uploaded_files:
                text = extract_text_from_pdf(f)
                texts.append(text)

        st.success("Text extracted. Building (or loading) vector index...")
        try:
            index = load_or_build_index(persist_dir=persist_dir, texts=texts)
            index_ready = True
            st.success("Index ready — ask your question below")
        except Exception as e:
            st.error(f"Failed to build index: {e}")


# Conversation state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if index_ready:
    user_question = st.text_input("Your question", key="question_input")
    if st.button("Ask") and user_question:
        with st.spinner("Retrieving relevant context and generating answer..."):
            # do retrieval
            try:
                # simple retrieval
                query_engine = index.as_query_engine()

                # Use Google generative as LLM via LLMPredictor adapter
                ggllm = GoogleGenerativeLLM(api_key=os.environ.get("GOOGLE_API_KEY"), model=model_choice)
                llm_predictor = LLMPredictor(LLMPredictorGoogle(ggllm, max_output_tokens=max_tokens))

                # Set predictor on the service context if needed
                # Note: depending on llama-index version, you may need to construct ServiceContext with llm_predictor
                # Here we call query with custom params if supported.

                response = query_engine.query(user_question)
                answer_text = str(response)
            except Exception as e:
                st.error(f"Error during query/generation: {e}")
                answer_text = None

            if answer_text:
                st.session_state.chat_history.append((user_question, answer_text))

    if st.session_state.chat_history:
        st.markdown("---")
        st.header("Conversation")
        for i, (q, a) in enumerate(reversed(st.session_state.chat_history)):
            st.markdown(f"**User:** {q}")
            st.markdown(f"**Assistant:** {a}")


st.markdown("---")
st.header("Deploy / Run")
st.markdown(
    "1. Save this file as `streamlit_app.py`.\n2. Ensure environment variable `GOOGLE_API_KEY` is set in Streamlit Cloud secrets or use a service account.\n3. Add `requirements.txt` with the packages listed in the top comment.\n4. Push to GitHub and connect the repo to Streamlit Cloud."
)

st.caption("This is a starter template — you may want to add caching, better error handling, more advanced prompt engineering, and support for larger PDFs by splitting into chunks.")
