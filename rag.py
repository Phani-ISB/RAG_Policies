# -*- coding: utf-8 -*-
"""RAG with policies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yupRh8EVZCDaIc_kFMdKIU0pcXL50nVr

# import all the dependencies
"""

import os
import streamlit as st
from pathlib import Path
from PyPDF2 import PdfReader
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Document
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.google_genai import GoogleGenAI
import tempfile


# --- Streamlit Page Config ---
st.set_page_config(page_title="ðŸ“š RAG Chatbot with LlamaIndex + Google API", layout="wide")
st.title("ðŸ’¬ RAG Chatbot (LlamaIndex + Google Generative AI)")
st.caption("Upload up to 2 PDFs. Uses FAISS + SentenceTransformers + Google Generative AI for RAG.")

# --- Ensure API key exists ---
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
llm = GoogleGenAI(model="models/gemini-2.0-flash")


# --- Directory to persist index ---
INDEX_DIR = Path("./index_store")
INDEX_DIR.mkdir(exist_ok=True)


# --- Helper: Load PDF and create Documents ---
def load_pdfs(uploaded_files):
    docs = []
    for file in uploaded_files:
        pdf_reader = PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() or ""
        docs.append(Document(text=text, metadata={"source": file.name}))
    return docs


# --- Build or Load Index ---
def get_or_create_index(docs):
    if INDEX_DIR.exists() and any(INDEX_DIR.iterdir()):
        storage_context = StorageContext.from_defaults(persist_dir=str(INDEX_DIR))
        index = load_index_from_storage(storage_context)
    else:
        embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
        index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)
        index.storage_context.persist(persist_dir=str(INDEX_DIR))
    return index


# --- Chat Interface ---
uploaded_files = st.file_uploader("Upload up to 2 PDF files", type=["pdf"], accept_multiple_files=True)
if uploaded_files:
    with st.spinner("ðŸ“„ Processing and indexing documents..."):
        docs = load_pdfs(uploaded_files)
        index = get_or_create_index(docs)
        st.success("âœ… Index built successfully!")

    # --- Google LLM via LlamaIndex ---
    llm = GoogleGenAI(model="gemini-2.0-flash", api_key=GOOGLE_API_KEY)

    query_engine = index.as_query_engine(llm=llm, similarity_top_k=3)

    # --- Chat Interface ---
    st.subheader("ðŸ’­ Ask a question about your PDFs")
    user_query = st.text_input("Enter your question:")

    if user_query:
        with st.spinner("ðŸ¤” Generating answer..."):
            response = query_engine.query(user_query)
            st.markdown("### ðŸ§  Answer:")
            st.write(response.response)
else:
    st.info("ðŸ‘† Please upload 1 or 2 PDFs to start.")
